{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance between Mexico and Thailand is approximately 16,000 kilometers (9,942 miles) when measured in a straight line.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country1} and {country2}?\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature=0.1,\n",
    "  openai_api_key=os.getenv(\"OPEN_API_KEY\"),\n",
    ")\n",
    "\n",
    "prompt = template.format(country1=\"Mexico\", country2=\"Thailand\")\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! Il mio nome è Paolo. La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri.')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "  SystemMessage(content=\"You are a geography expert. And you only reply in Italian\"),\n",
    "  AIMessage(content=\"Ciao, mi chiamo Paolo\"),\n",
    "  HumanMessage(content=\"What is the distance between the Mexico and Thailand? Also what is your name?\"),\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Γεια σας! Η απόσταση μεταξύ του Μεξικού και της Ταϊλάνδης είναι περίπου 16.000 χιλιόμετρα. Το όνομά μου είναι Σωκράτης. Πώς μπορώ να βοηθήσω;')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "      (\"ai\", \"Ciao, mi chiamo {name}!\"),\n",
    "      (\"human\", \"What is the distance between {country_a} and {country_b}. Also, what is your name?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Socrates\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'my name', 'is John']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "  def parse(self, text):\n",
    "    items = text.strip().split(\",\")\n",
    "    return list(map(str.strip, items))\n",
    "  \n",
    "  \n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(\"Hello, my name, is John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red, blue, green, yellow, orange, purple, pink, black, white, brown"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a list generating machine. Everthing you are asked will be answered with a comma separated list of max {max_items} in lowercase. DON'T reply with anything else.\"),\n",
    "  (\"human\", \"{question}\"),\n",
    "])\n",
    "                                            \n",
    "prompt = template.format_messages(\n",
    "  max_items=10,\n",
    "  question=\"What are the colors?\",\n",
    ")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pikachu, charmander, bulbasaur, squirtle, jigglypuff"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charmander', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "  \"max_items\": 5,\n",
    "  \"question\": \"What are the pokemons?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature = 0.1,\n",
    "  streaming = True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()],\n",
    "  openai_api_key=os.getenv(\"OPEN_API_KEY\")\n",
    ")\n",
    "\n",
    "chef_template = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a world-class international chef. You are asked to create a recipe with easy to find ingredients.\"),\n",
    "  (\"human\", \"I want to cook {cuisine} food.\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_template | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! How about trying a classic Indian dish like Chicken Tikka Masala? It's a flavorful and aromatic dish that is sure to impress your friends and family. Here's a simple recipe for you to try:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb boneless, skinless chicken breasts, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 tablespoons vegetable oil\n",
      "- 1 onion, finely chopped\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon ginger, minced\n",
      "- 1 can (14 oz) diced tomatoes\n",
      "- 2 tablespoons tomato paste\n",
      "- 1 tablespoon garam masala\n",
      "- 1 teaspoon ground cumin\n",
      "- 1 teaspoon ground coriander\n",
      "- 1 teaspoon paprika\n",
      "- 1/2 teaspoon turmeric\n",
      "- 1/2 teaspoon cayenne pepper (adjust to taste)\n",
      "- Salt and pepper to taste\n",
      "- 1/2 cup heavy cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, mix together the yogurt, lemon juice, 1 tablespoon of vegetable oil, half of the minced garlic, half of the minced ginger, and a pinch of salt. Add the chicken pieces and marinate for at least 1 hour, preferably overnight.\n",
      "2. Preheat the oven to 400°F (200°C). Thread the marinated chicken onto skewers and place them on a baking sheet. Bake for 20-25 minutes or until the chicken is cooked through.\n",
      "3. In a large skillet, heat the remaining tablespoon of vegetable oil over medium heat. Add the chopped onion and cook until softened, about 5 minutes.\n",
      "4. Add the remaining garlic and ginger to the skillet and cook for another minute until fragrant.\n",
      "5. Stir in the diced tomatoes, tomato paste, garam masala, cumin, coriander, paprika, turmeric, cayenne pepper, salt, and pepper. Cook for 5-7 minutes, stirring occasionally.\n",
      "6. Add the baked chicken pieces to the skillet and stir to coat them in the sauce. Simmer for another 10 minutes.\n",
      "7. Stir in the heavy cream and cook for an additional 5 minutes.\n",
      "8. Serve the Chicken Tikka Masala over steamed rice or with naan bread. Garnish with chopped cilantro before serving.\n",
      "\n",
      "Enjoy your homemade Chicken Tikka Masala!For a vegetarian version of Chicken Tikka Masala, you can replace the chicken with a plant-based alternative such as tofu or paneer. Here's how you can prepare these alternatives:\n",
      "\n",
      "1. Tofu: Use firm or extra-firm tofu for this recipe. Drain the tofu and press it to remove excess water. Cut the tofu into bite-sized cubes and follow the marinating instructions in the recipe. Tofu absorbs flavors well, so marinating it for a longer time will enhance the taste. Instead of baking, you can pan-fry the marinated tofu until it's golden brown and slightly crispy.\n",
      "\n",
      "2. Paneer: Paneer is a type of Indian cheese that holds its shape well when cooked. You can either make your own paneer at home or buy it from the store. Cut the paneer into bite-sized cubes and follow the marinating instructions in the recipe. Since paneer is already firm, you can skip the baking step and directly add the marinated paneer to the sauce in the skillet. Cook it gently to allow the flavors to meld together.\n",
      "\n",
      "By using tofu or paneer as a substitute for chicken, you can enjoy a delicious vegetarian version of Chicken Tikka Masala that retains the essence of the traditional dish."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"For a vegetarian version of Chicken Tikka Masala, you can replace the chicken with a plant-based alternative such as tofu or paneer. Here's how you can prepare these alternatives:\\n\\n1. Tofu: Use firm or extra-firm tofu for this recipe. Drain the tofu and press it to remove excess water. Cut the tofu into bite-sized cubes and follow the marinating instructions in the recipe. Tofu absorbs flavors well, so marinating it for a longer time will enhance the taste. Instead of baking, you can pan-fry the marinated tofu until it's golden brown and slightly crispy.\\n\\n2. Paneer: Paneer is a type of Indian cheese that holds its shape well when cooked. You can either make your own paneer at home or buy it from the store. Cut the paneer into bite-sized cubes and follow the marinating instructions in the recipe. Since paneer is already firm, you can skip the baking step and directly add the marinated paneer to the sauce in the skillet. Cook it gently to allow the flavors to meld together.\\n\\nBy using tofu or paneer as a substitute for chicken, you can enjoy a delicious vegetarian version of Chicken Tikka Masala that retains the essence of the traditional dish.\")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 풀스택 GPT 3.4 (20241209)\n",
    "\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it. \"),\n",
    "  (\"human\", \"{recipe}\"),\n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "  \"cuisine\": \"Indian\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "   Here is what I know:\n",
      "   Capital: Ankara\n",
      "   Language: Turkish\n",
      "   Currency: Turkish Lira\n",
      "   Food: Kebab"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI: \\n   Here is what I know:\\n   Capital: Ankara\\n   Language: Turkish\\n   Currency: Turkish Lira\\n   Food: Kebab')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 풀스택 GPT 4.1 (20241210)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature = 0.1,\n",
    "  streaming = True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()],\n",
    "  openai_api_key=os.getenv(\"OPEN_API_KEY\")\n",
    ")\n",
    "\n",
    "examples = [\n",
    "  {\"question\": \"What do you know about France?\",\n",
    "   \"answer\": \"\"\"\n",
    "   Here is what I know\n",
    "   Capital: Paris\n",
    "   Language: French\n",
    "   Currency: Euro\n",
    "   Food: French Fries\n",
    "   \"\"\"\n",
    "  },\n",
    "  {\"question\": \"What do you know about Germany?\",\n",
    "   \"answer\": \"\"\"\n",
    "   I know this:\n",
    "   Capital: Berlin\n",
    "   Language: German\n",
    "   Currency: Euro\n",
    "   Food: Bratwurst\n",
    "   \"\"\"\n",
    "  },\n",
    "  {\"question\": \"What do you know about Italy?\",\n",
    "   \"answer\": \"\"\"\n",
    "   Here is what I know\n",
    "   Capital: Rome\n",
    "   Language: Italian\n",
    "   Currency: Euro\n",
    "   Food: Pizza\n",
    "   \"\"\"\n",
    "  },\n",
    "]\n",
    "\n",
    "# same variables with examples\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "  example_prompt=example_prompt,\n",
    "  examples=examples,\n",
    "  suffix=\"Human: What do you know about {country}?\",\n",
    "  input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "  \"country\": \"Turkey\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Here is what I know\n",
      "   Capital: Bangkok\n",
      "   Language: Thai\n",
      "   Currency: Thai Baht\n",
      "   Food: Pad Thai"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n   Here is what I know\\n   Capital: Bangkok\\n   Language: Thai\\n   Currency: Thai Baht\\n   Food: Pad Thai')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#풀스택 GPT 4.2 (20241210)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, ChatMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature = 0.1,\n",
    "  streaming = True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()],\n",
    "  openai_api_key=os.getenv(\"OPEN_API_KEY\")\n",
    ")\n",
    "\n",
    "examples = [\n",
    "  {\"country\": \"France\",\n",
    "   \"answer\": \"\"\"\n",
    "   Here is what I know\n",
    "   Capital: Paris\n",
    "   Language: French\n",
    "   Currency: Euro\n",
    "   Food: French Fries\n",
    "   \"\"\"\n",
    "  },\n",
    "  {\"country\": \"Germany\",\n",
    "   \"answer\": \"\"\"\n",
    "   I know this:\n",
    "   Capital: Berlin\n",
    "   Language: German\n",
    "   Currency: Euro\n",
    "   Food: Bratwurst\n",
    "   \"\"\"\n",
    "  },\n",
    "  {\"country\": \"Italy\",\n",
    "   \"answer\": \"\"\"\n",
    "   Here is what I know\n",
    "   Capital: Rome\n",
    "   Language: Italian\n",
    "   Currency: Euro\n",
    "   Food: Pizza\n",
    "   \"\"\"\n",
    "  },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"human\", \"What do you know about {country}\"),\n",
    "  (\"ai\", \"{answer}\"),\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "  example_prompt=example_prompt,\n",
    "  examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a geography expert. You have to give me short answer.\"),\n",
    "  example_prompt,\n",
    "  (\"human\", \"What do you know about {country}?\"),\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "  \"country\": \"Thailand\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about Italy?\\nAI: \\n   Here is what I know\\n   Capital: Rome\\n   Language: Italian\\n   Currency: Euro\\n   Food: Pizza\\n   \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#풀스택 GPT 4.3 (20241210)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, ChatMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "import os\n",
    "    \n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature = 0.1,\n",
    "  streaming = True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()],\n",
    "  openai_api_key=os.getenv(\"OPEN_API_KEY\")\n",
    ")\n",
    "\n",
    "examples = [\n",
    "  {\"question\": \"What do you know about France?\",\n",
    "   \"answer\": \"\"\"\n",
    "   Here is what I know\n",
    "   Capital: Paris\n",
    "   Language: French\n",
    "   Currency: Euro\n",
    "   Food: French Fries\n",
    "   \"\"\"\n",
    "  },\n",
    "  {\"question\": \"What do you know about Germany?\",\n",
    "   \"answer\": \"\"\"\n",
    "   I know this:\n",
    "   Capital: Berlin\n",
    "   Language: German\n",
    "   Currency: Euro\n",
    "   Food: Bratwurst\n",
    "   \"\"\"\n",
    "  },\n",
    "  {\"question\": \"What do you know about Italy?\",\n",
    "   \"answer\": \"\"\"\n",
    "   Here is what I know\n",
    "   Capital: Rome\n",
    "   Language: Italian\n",
    "   Currency: Euro\n",
    "   Food: Pizza\n",
    "   \"\"\"\n",
    "  },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "  \n",
    "  def __init__(self, examples):\n",
    "    self.examples = examples\n",
    "    \n",
    "  def add_example(self, example):\n",
    "    self.examples.append(example)\n",
    "  \n",
    "  def select_examples(self, input_variables):\n",
    "    from random import choice\n",
    "    return [choice(self.examples)]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "  examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "  example_prompt=example_prompt,\n",
    "  example_selector=example_selector,\n",
    "  suffix=\"Human: What do you know about {country}?\",\n",
    "  input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr matey! Me favorite grub be a hearty plate o' salted beef and hardtack! Aye, nothing beats the taste o' the sea in me belly! Arrr!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"Arrr matey! Me favorite grub be a hearty plate o' salted beef and hardtack! Aye, nothing beats the taste o' the sea in me belly! Arrr!\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#풀스택 GPT 4.4 (20241210)\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "# from langchain.prompts import load_prompt\n",
    "\n",
    "# prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature = 0.1,\n",
    "  streaming = True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()],\n",
    "  openai_api_key=os.getenv(\"OPEN_API_KEY\")\n",
    ")\n",
    "\n",
    "# prompt.format(country=\"Germany\")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "  final_prompt=final, \n",
    "  pipeline_prompts=prompts\n",
    ")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make Italian pasta\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [5.22s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 13,\n",
      "      \"completion_tokens\": 254,\n",
      "      \"total_tokens\": 267,\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"cached_tokens\": 0,\n",
      "        \"audio_tokens\": 0\n",
      "      },\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and create a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#풀스택 GPT 4.5 (20241210)\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "  temperature = 0.1,\n",
    "  # streaming = True,\n",
    "  # callbacks=[StreamingStdOutCallbackHandler()],\n",
    "  openai_api_key=os.getenv(\"OPEN_API_KEY\")\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make Italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kpkim/Documents/Fullstack GPT/venv/lib/python3.11/site-packages/langchain/llms/openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIChat\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# chat = OpenAI(\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#   temperature = 0.1,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#   openai_api_key=os.getenv(\"OPEN_API_KEY\"),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# chat.save(\"model.json\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m OpenAI(openai_api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPEN_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m chat\u001b[38;5;241m=\u001b[39m\u001b[43mload_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m chat\n",
      "File \u001b[0;32m~/Documents/Fullstack GPT/venv/lib/python3.11/site-packages/langchain/llms/loading.py:44\u001b[0m, in \u001b[0;36mload_llm\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile type must be json or yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Load the LLM from the config now.\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_llm_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Fullstack GPT/venv/lib/python3.11/site-packages/langchain/llms/loading.py:24\u001b[0m, in \u001b[0;36mload_llm_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LLM not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m llm_cls \u001b[38;5;241m=\u001b[39m type_to_cls_dict[config_type]()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Fullstack GPT/venv/lib/python3.11/site-packages/langchain/llms/openai.py:221\u001b[0m, in \u001b[0;36mBaseOpenAI.__new__\u001b[0;34m(cls, **data)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    214\u001b[0m     model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to use a chat model. This way of initializing it is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno longer supported. Instead, please use: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from langchain.chat_models import ChatOpenAI`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOpenAIChat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Fullstack GPT/venv/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Documents/Fullstack GPT/venv/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIChat\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "#풀스택 GPT 4.5 (20241210)\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = OpenAI(\n",
    "#   temperature = 0.1,\n",
    "#   openai_api_key=os.getenv(\"OPEN_API_KEY\"),\n",
    "#   max_tokens=450,\n",
    "#   model=\"gpt-3.5-turbo-16k\"\n",
    "# )\n",
    "\n",
    "# chat.save(\"model.json\")\n",
    "chat=load_llm(\"model.json\")\n",
    "\n",
    "chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
